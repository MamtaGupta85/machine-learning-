# -*- coding: utf-8 -*-
"""customer_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1usPf0wu24AklJEPM9Jfrynn43nhQWv54
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,VotingClassifier
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

df_cust = pd.read_csv('/content/drive/MyDrive/Machine Learning/Churn_Modelling.csv')

df_cust.head()

#dimensions of the dataset
df_cust.shape

#dropping the columns of row number, customer_ID and surname
df_cust.drop(columns=['RowNumber','CustomerId','Surname'],axis=1,inplace=True)

#information of the dataset
df_cust.info()

#@title Exploratory Data Analysis and Pre processing
df_cust.describe().T

#check for the null values
df_cust.isna().sum()

#total values for the interested columns
cols =['Gender','HasCrCard','IsActiveMember','Exited','Tenure']
for col in df_cust[cols]:
  col = df_cust.groupby(col)[col].value_counts()
  print(f'the value counts for {col}')
  print('============================================')

#@title Univariate Analysis
plt.rcParams['figure.figsize']=(8,6)
df_cust['Gender'].value_counts().plot.pie(autopct='%1.f%%',explode=[0.1,0])
plt.title('Gender distribution')
plt.show()

def dist_plot(x,fig):
  plt.subplot(2,2,fig)
  sns.distplot(df_cust[x],bins=10,color='r')
  plt.xlabel(x+' distribution')

plt.rcParams['figure.figsize']=(10,6)
dist_plot('Age',1)
dist_plot('CreditScore',2)
dist_plot('Balance',3)
dist_plot('EstimatedSalary',4)

#@title Multivariate Analysis
def box_d(x,fig):
  plt.subplot(2,2,fig)
  sns.boxplot(x=df_cust[x],y=df_cust['CreditScore'],hue=df_cust['Exited'],palette='magma')
  plt.title(x+' boxplot')
  plt.show()

box_d('Gender',1)
box_d('IsActiveMember',2)
box_d('NumOfProducts',3)
box_d('Geography',4)

#deal with the outliers
df_cust[(df_cust['CreditScore']<380) & (df_cust['Exited']==1)]

df_cust.drop([7,942,1193,1405,1631,1838,1962,2473,2579,8154,8723,8762,9356,9624],inplace=True)

#categorical values in the dataset
cat = df_cust.dtypes==object
cat_features = df_cust.columns[cat].tolist()

cat_features

#encode the categorical values
lb = LabelEncoder()
df_cust['Gender']=lb.fit_transform(df_cust['Gender'])
df_cust['Geography']=lb.fit_transform(df_cust['Geography'])

#correlation analysis
x= df_cust
sns.heatmap(df_cust.corr(), annot=True,cmap='magma')

#checking for the multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

VIF_data = pd.DataFrame()
v = df_cust
VIF_data['Features']=x.columns
VIF_data["VIF_factor"]= [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]

VIF_data

#dropping columns
df_cust.drop(columns=['CreditScore','HasCrCard','Tenure'],inplace=True)

#use SMOTE
smote = SMOTE()
input = df_cust.drop(columns=['Exited'])
output = df_cust['Exited']
x_smote, y_smote = smote.fit_resample(input,output)

print(x_smote.shape)
print(y_smote.shape)

#splitting the data for training and testing
train_x,test_x,train_y,test_y = train_test_split(x_smote,y_smote,test_size=0.2)

#scale down the testing and training data
rc = RobustScaler()
train_x = rc.fit_transform(train_x)
test_x = rc.fit_transform(test_x)

#@title Logistic Regression
lr_cust = LogisticRegression()
lr_cust.fit(train_x,train_y)

#predicting the value of y
y_pred = lr_cust.predict(test_x)

#check the accuracy of the model
print(f'the accuracy of the model is {accuracy_score(test_y,y_pred)}')

print(classification_report(test_y,y_pred))



#@title Decision Tree Classification
dt = DecisionTreeClassifier()
dt.fit(train_x,train_y)

#predict the value of y
y_dt_pred = dt.predict(test_x)

#checking the accuracy score
print(f'the accuracy of the decision tree model is {accuracy_score(test_y, y_dt_pred)*100:.2f}%')

print(classification_report(test_y,y_dt_pred))

#Confusion matrix
conf_matrix = confusion_matrix(test_y,y_dt_pred)

plt.rcParams['figure.figsize']=(12,6)
ConfusionMatrixDisplay(confusion_matrix=conf_matrix).plot()

#@title Support vector Machines

svm_cust = SVC(kernel='rbf',random_state=0,gamma=.10,C=1.0)
svm_cust.fit(train_x,train_y)

pred_cust = svm_cust.predict(test_x)

#printing the score of SVM
svm_cust.score(test_x,test_y)

svm_cust.score(train_x,train_y)

#@title Ensembling techniques

kfold=KFold(n_splits=10)
model_list = [RandomForestClassifier(n_estimators=50), GradientBoostingClassifier(n_estimators=50), AdaBoostClassifier(n_estimators=50)]

model_name_list=[]
accuracy_list=[]

for model_name in model_list:
  model = model_name.fit(train_x,train_y)

  model_cv = cross_val_score(model,x_smote,y_smote, cv=kfold, scoring='accuracy')

  model_name_list.append(model_name.__class__.__name__)
  accuracy_list.append(model_cv.mean())

  print(f'{model_name.__class__.__name__} cross validation score is {model_cv.mean()*100:.2f}%')
  print('==='*8)